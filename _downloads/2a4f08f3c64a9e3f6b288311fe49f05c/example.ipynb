{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Usage Examples\n\nThese examples demonstrate a simple usage of the BDE package both for\nregression and classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport os\nimport sys\n\nimport pandas as pd\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \"../\")))\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n)\nlogging.getLogger(\"bde\").setLevel(logging.INFO)\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.scipy.stats import norm\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom bde import BdeClassifier, BdeRegressor\nfrom bde.loss.loss import CategoricalCrossEntropy, GaussianNLL\nfrom bde.sampler.prior import PriorDist\n\n\ndef regression_airfoil_example():\n    print(\"-\" * 20)\n    print(\"Regression example\")\n    print(\"-\" * 20)\n\n    data = pd.read_csv(\"bde/data/airfoil.csv\", sep=\",\", header=0)\n\n    X = data.iloc[:, :-1].values  # (1503, 5)\n    y = data.iloc[:, -1].values  # (1503,)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    x_scaler = StandardScaler()\n    y_scaler = StandardScaler()\n\n    X_train = x_scaler.fit_transform(X_train)\n    X_test = x_scaler.transform(X_test)\n\n    y_train = y_scaler.fit_transform(y_train.reshape(-1, 1)).ravel()\n    y_test = y_scaler.transform(y_test.reshape(-1, 1)).ravel()\n\n    regressor = BdeRegressor(\n        hidden_layers=[16, 16],\n        n_members=8,\n        seed=0,\n        loss=GaussianNLL(),\n        epochs=1000,\n        validation_split=0.15,\n        lr=1e-3,\n        weight_decay=1e-3,\n        warmup_steps=50000,  # 50k in the original paper\n        n_samples=10000,  # 10k in the original paper\n        n_thinning=10,\n        patience=20,\n    )\n\n    print(f\"the params are {regressor.get_params()}\")  # get_params is from sklearn!\n    regressor.fit(x=jnp.array(X_train), y=jnp.array(y_train))\n\n    means, sigmas = regressor.predict(jnp.array(X_test), mean_and_std=True)\n\n    print(\"RSME: \", root_mean_squared_error(y_true=y_test, y_pred=means))\n    mean, intervals = regressor.predict(X_test, credible_intervals=[0.1, 0.9])\n    raw = regressor.predict(X_test, raw=True)\n    print(\n        f\"The shape of the raw predictions are {raw.shape}\"\n    )  # (ensemble members, n_samples/n_thinning, n_data, (mu,sigma))\n\n    # use the raw predictions to compute log pointwise predictive density (lppd)\n    n_data = y_test.shape[0]\n    log_likelihoods = norm.logpdf(\n        y_test.reshape(1, 1, n_data),\n        loc=raw[:, :, :, 0],\n        scale=jax.nn.softplus(raw[..., 1]) + 1e-6,  # map raw scale via softplus\n    )  # (E,T,N)\n    b = 1 / jnp.prod(jnp.array(log_likelihoods.shape[:-1]))  # 1/ET\n    axis = tuple(range(len(log_likelihoods.shape) - 1))\n    log_likelihoods = jax.scipy.special.logsumexp(log_likelihoods, b=b, axis=axis)\n    lppd = jnp.mean(log_likelihoods)\n    print(f\"The log pointwise predictive density (lppd) is {lppd}\")\n\n    print(\"Quantiles shape:\", intervals.shape)  # (len(q), N)\n    # calculate the coverage of the 80% credible interval\n    lower = intervals[0]\n    upper = intervals[1]\n    coverage = jnp.mean((y_test.ravel() >= lower) & (y_test.ravel() <= upper))\n    print(f\"Coverage of the 80% credible interval: {coverage * 100:.2f}%\")\n\n    score = regressor.score(X_test, y_test)\n    print(f\"The sklearn test score is {score}\")\n\n    # print(f\"This is the history of the regressor\\n {regressor.history()}\")\n\n\ndef classification_example():\n    print(\"-\" * 20)\n    print(\"Classification example\")\n    print(\"-\" * 20)\n    iris = load_iris()\n    X = iris.data.astype(\"float32\")\n    y = iris.target.astype(\"int32\").ravel()  # 0, 1, 2\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    classifier = BdeClassifier(\n        n_members=4,\n        hidden_layers=[16, 16],\n        seed=0,\n        loss=CategoricalCrossEntropy(),\n        activation=\"relu\",\n        epochs=1000,\n        validation_split=0.15,\n        lr=1e-3,\n        warmup_steps=400,  # very few steps required for this simple dataset\n        n_samples=100,\n        n_thinning=1,\n        patience=10,\n    )\n\n    classifier.fit(x=X_train, y=y_train)\n\n    preds = classifier.predict(X_test)\n    probs = classifier.predict_proba(X_test)\n    print(\"Predicted class probabilities shape:\\n\", probs.shape)\n    accuracy = jnp.mean(preds == y_test)\n    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n    score = classifier.score(X_train, y_train)\n    print(f\"The sklearn score is {score}\")\n    raw = classifier.predict(X_test, raw=True)\n    print(\n        f\"The shape of the raw predictions are {raw.shape}\"\n    )  # (ensemble members, n_samples/n_thinning, n_test_data, n_classes)\n\n\ndef regression_concrete_example():\n    print(\"-\" * 20)\n    print(\"Regression example for concrete data\")\n    print(\"-\" * 20)\n\n    data = pd.read_csv(\"bde/data/concrete.data\", sep=\" \", header=None)\n\n    X = data.iloc[:, :-1].values  # (1030, 8)\n    y = data.iloc[:, -1].values  # (1030,)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    x_scaler = StandardScaler()\n    y_scaler = StandardScaler()\n\n    X_train = x_scaler.fit_transform(X_train)\n    X_test = x_scaler.transform(X_test)\n\n    y_train = y_scaler.fit_transform(y_train.reshape(-1, 1)).ravel()\n    y_test = y_scaler.transform(y_test.reshape(-1, 1)).ravel()\n\n    regressor = BdeRegressor(\n        hidden_layers=[16, 16],\n        n_members=8,\n        seed=0,\n        loss=GaussianNLL(),\n        epochs=1000,\n        validation_split=0.15,\n        lr=1e-3,\n        weight_decay=1e-4,\n        warmup_steps=50000,\n        n_samples=10000,\n        n_thinning=0,\n        patience=10,\n        prior_family=PriorDist.NORMAL,\n        prior_kwargs={\"loc\": 0, \"scale\": 1.5},\n    )\n\n    print(f\"The params are {regressor.get_params()}\")\n    regressor.fit(x=jnp.array(X_train), y=jnp.array(y_train))\n\n    means, sigmas = regressor.predict(jnp.array(X_test), mean_and_std=True)\n    rmse = root_mean_squared_error(y_true=y_test, y_pred=means)\n    print(\"RMSE:\", rmse)\n    mean, intervals = regressor.predict(X_test, credible_intervals=[0.1, 0.9])\n    raw = regressor.predict(X_test, raw=True)\n    print(\n        f\"The shape of the raw predictions are {raw.shape}\"\n    )  # (ensemble members, n_samples/n_thinning, n_data, (mu,sigma))\n\n    # use the raw predictions to compute log pointwise predictive density (lppd)\n    n_data = y_test.shape[0]\n    log_likelihoods = norm.logpdf(\n        y_test.reshape(1, 1, n_data),\n        loc=raw[:, :, :, 0],\n        scale=jax.nn.softplus(raw[..., 1]) + 1e-6,  # map raw scale via softplus\n    )  # (E,T,N)\n    b = 1 / jnp.prod(jnp.array(log_likelihoods.shape[:-1]))  # 1/ET\n    axis = tuple(range(len(log_likelihoods.shape) - 1))\n    log_likelihoods = jax.scipy.special.logsumexp(log_likelihoods, b=b, axis=axis)\n    lppd = jnp.mean(log_likelihoods)\n    print(f\"The log pointwise predictive density (lppd) is {lppd}\")\n\n    print(\"Quantiles shape:\", intervals.shape)  # (len(q), N)\n    # calculate the coverage of the 80% credible interval\n    lower = intervals[0]\n    upper = intervals[1]\n    coverage = jnp.mean((y_test.ravel() >= lower) & (y_test.ravel() <= upper))\n    print(f\"Coverage of the 80% credible interval: {coverage * 100:.2f}%\")\n\n    score = regressor.score(X_test, y_test)\n    print(f\"The sklearn score is {score}\")\n\n\ndef sklearn_pipeline():\n    print(\"-\" * 20)\n    print(\"Sklearn Pipeline with GridSearchCV example\")\n    print(\"-\" * 20)\n\n    base_estimator = BdeClassifier(\n        n_members=4,\n        hidden_layers=[16, 16],\n        seed=0,\n        loss=CategoricalCrossEntropy(),\n        activation=\"relu\",\n        epochs=100,\n        lr=1e-3,\n        warmup_steps=400,  # very few steps required for this simple dataset\n        n_samples=100,\n        n_thinning=1,\n        patience=10,\n    )\n\n    pipe = Pipeline(\n        [\n            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n            (\"clf\", base_estimator),\n        ]\n    )\n\n    # Explore relevant BDE hyperparameters while keeping the search lightweight.\n    param_grid = {\n        \"clf__n_members\": [4],\n        \"clf__hidden_layers\": [[8, 8], [16, 16]],\n        \"clf__epochs\": [20],\n        \"clf__lr\": [1e-3],\n        \"clf__warmup_steps\": [400],\n        \"clf__n_samples\": [100],\n        \"scaler__with_std\": [True],\n    }\n\n    grid = GridSearchCV(\n        estimator=pipe,\n        param_grid=param_grid,\n        scoring={\"accuracy\": \"accuracy\", \"f1_macro\": \"f1_macro\"},\n        refit=\"f1_macro\",\n        n_jobs=None,  # single-threaded for clarity; set to -1 if you want parallel\n        cv=3,\n        verbose=1,\n        return_train_score=True,\n    )\n    iris = load_iris()\n    X = iris.data.astype(\"float32\")\n    y = iris.target.astype(\"int32\").ravel()  # 0, 1, 2\n\n    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n    grid.fit(X_train, y_train)\n\n    print(\"Best params:\", grid.best_params_)\n    print(\"Best CV f1_macro: {:.4f}\".format(grid.best_score_))\n\n\nif __name__ == \"__main__\":\n    regression_airfoil_example()\n    #classification_example()\n    #regression_concrete_example()\n    #sklearn_pipeline()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}