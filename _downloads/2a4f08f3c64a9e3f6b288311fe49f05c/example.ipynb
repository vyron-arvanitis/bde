{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Usage Examples\n\nThese examples demonstrate a simple usage of the BDE package both for\nregression and classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \"../\")))\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n)\nlogging.getLogger(\"bde\").setLevel(logging.INFO)\n\nimport jax\nimport jax.numpy as jnp\nfrom sklearn.datasets import fetch_openml, load_iris\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom bde import BdeClassifier, BdeRegressor\nfrom bde.loss.loss import CategoricalCrossEntropy, GaussianNLL\n\n\ndef regression_example():\n    print(\"-\" * 20)\n    print(\"Regression example\")\n    print(\"-\" * 20)\n    data = fetch_openml(name=\"airfoil_self_noise\", as_frame=True)\n\n    X = data.data.values  # shape (1503, 5)\n    y = data.target.values.reshape(-1, 1)  # shape (1503, 1)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    Xmu, Xstd = jnp.mean(X_train, 0), jnp.std(X_train, 0) + 1e-8\n    Ymu, Ystd = jnp.mean(y_train, 0), jnp.std(y_train, 0) + 1e-8\n\n    Xtr = (X_train - Xmu) / Xstd\n    Xte = (X_test - Xmu) / Xstd\n    ytr = (y_train - Ymu) / Ystd\n    yte = (y_test - Ymu) / Ystd\n\n    regressor = BdeRegressor(\n        hidden_layers=[16, 16],\n        n_members=8,\n        seed=0,\n        loss=GaussianNLL(),\n        epochs=200,\n        lr=1e-3,\n        warmup_steps=5000,  # 50k in the original paper\n        n_samples=2000,  # 10k in the original paper\n        n_thinning=2,\n        patience=10,\n    )\n\n    print(f\"the params are {regressor.get_params()}\")  # get_params is from sklearn!\n    regressor.fit(x=Xtr, y=ytr)\n\n    means, sigmas = regressor.predict(Xte, mean_and_std=True)\n\n    print(\"RSME: \", root_mean_squared_error(y_true=yte, y_pred=means))\n    mean, intervals = regressor.predict(Xte, credible_intervals=[0.1, 0.9])\n    raw = regressor.predict(Xte, raw=True)\n    print(\n        f\"The shape of the raw predictions are {raw.shape}\"\n    )  # (ensemble members, n_samples, n_data, (mu,sigma))\n\n    # use the raw predictions to compute log pointwise predictive density (lppd)\n    from jax.scipy.stats import norm\n\n    n_data = yte.shape[0]\n    log_likelihoods = norm.logpdf(\n        yte.reshape(1, 1, n_data),\n        loc=raw[:, :, :, 0],\n        scale=jnp.exp(raw[..., 1]).clip(min=1e-6, max=1e6),  # note we model log sigma\n    )\n    b = 1 / jnp.prod(jnp.array(log_likelihoods.shape[:-1]))\n    axis = tuple(range(len(log_likelihoods.shape) - 1))\n    log_likelihoods = jax.scipy.special.logsumexp(log_likelihoods, b=b, axis=axis)\n    lppd = jnp.mean(log_likelihoods)\n    print(f\"The log pointwise predictive density (lppd) is {lppd}\")\n\n    print(\"Quantiles shape:\", intervals.shape)  # (len(q), N)\n    # calculate the coverage of the 80% credible interval\n    lower = intervals[0]\n    upper = intervals[1]\n    coverage = jnp.mean((yte.ravel() >= lower) & (yte.ravel() <= upper))\n    print(f\"Coverage of the 80% credible interval: {coverage * 100:.2f}%\")\n\n    score = regressor.score(Xtr, ytr)\n    print(f\"The sklearn score is {score}\")\n\n\ndef classification_example():\n    print(\"-\" * 20)\n    print(\"Classification example\")\n    print(\"-\" * 20)\n    iris = load_iris()\n    X = iris.data.astype(\"float32\")\n    y = iris.target.astype(\"int32\").ravel()  # 0, 1, 2\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    classifier = BdeClassifier(\n        n_members=4,\n        hidden_layers=[16, 16],\n        seed=0,\n        loss=CategoricalCrossEntropy(),\n        activation=\"relu\",\n        epochs=100,\n        lr=1e-3,\n        warmup_steps=400,  # very few steps required for this simple dataset\n        n_samples=100,\n        n_thinning=1,\n        patience=10,\n    )\n\n    classifier.fit(x=X_train, y=y_train)\n\n    preds = classifier.predict(X_test)\n    probs = classifier.predict_proba(X_test)\n    print(\"Predicted class probabilities shape:\\n\", probs.shape)\n    accuracy = jnp.mean(preds == y_test)\n    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n    score = classifier.score(X_train, y_train)\n    print(f\"The sklearn score is {score}\")\n    raw = classifier.predict(X_test, raw=True)\n    print(\n        f\"The shape of the raw predictions are {raw.shape}\"\n    )  # (ensemble members, n_samples, n_test_data, n_classes))\n\n\nif __name__ == \"__main__\":\n    classification_example()\n    regression_example()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}